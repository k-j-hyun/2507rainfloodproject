import os
import time
import requests
import pandas as pd
import datetime
from dotenv import load_dotenv

load_dotenv('.env')

# ì„¤ì •
SERVICE_KEY = os.getenv("OPENWEATHER_API_KEY")
BASE_DIR = "data"
os.makedirs(BASE_DIR, exist_ok=True)

# ê³µí†µ ì„¤ì •
STN_ID = 108
NUM_OF_ROWS = 800
START_DATE = datetime.date(2000, 1, 10)
END_DATE = datetime.date.today() - datetime.timedelta(days=1)

# 1. ì‹œê°„ë³„ ê´€ì¸¡ìë£Œ ìˆ˜ì§‘ (ASOS)
def get_hourly_data():
    """ì‹œê°„ë³„ ë°ì´í„° ìˆ˜ì§‘"""
    CSV_FILE = os.path.join(BASE_DIR, 'asos_seoul_hourly.csv')
    API_URL = "http://apis.data.go.kr/1360000/AsosHourlyInfoService/getWthrDataList"
    
    def load_existing_times():
        """ê¸°ì¡´ì— ìˆ˜ì§‘ëœ ì‹œê°„ ë°ì´í„° ë¡œë“œ"""
        if not os.path.exists(CSV_FILE):
            return set()
        try:
            df = pd.read_csv(CSV_FILE)
            return set(df['tm'].astype(str))
        except Exception as e:
            print(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return set()

    def fetch_day_data(date_obj, existing_times):
        """ì§€ì •ëœ í•˜ë£¨ì¹˜(00~23ì‹œ) ë°ì´í„°ë¥¼ ìˆ˜ì§‘"""
        all_results = []
        start_dt = datetime.datetime.combine(date_obj, datetime.time(0, 0))
        end_dt = datetime.datetime.combine(date_obj, datetime.time(23, 0))

        page = 1
        while True:
            params = {
                'serviceKey': SERVICE_KEY,
                'numOfRows': NUM_OF_ROWS,
                'pageNo': page,
                'dataCd': 'ASOS',
                'dateCd': 'HR',
                'startDt': start_dt.strftime("%Y%m%d"),
                'startHh': "00",
                'endDt': end_dt.strftime("%Y%m%d"),
                'endHh': "23",
                'stnIds': STN_ID,
                'dataType': 'JSON'
            }

            try:
                response = requests.get(API_URL, params=params, timeout=60)
                response.raise_for_status()
            except requests.exceptions.RequestException as e:
                print(f"[{date_obj}] ìš”ì²­ ì‹¤íŒ¨: {e}")
                return []

            try:
                data = response.json()
                result_code = data['response']['header']['resultCode']
                result_msg = data['response']['header']['resultMsg']
                if result_code != '00':
                    print(f"[{date_obj}] API ì˜¤ë¥˜: {result_msg}")
                    return []
            except Exception as e:
                print(f"[{date_obj}] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                return []

            items = data['response']['body']['items'].get('item', [])
            if not items:
                break

            # ìƒˆë¡œìš´ ë°ì´í„°ë§Œ ì¶”ê°€
            for item in items:
                if item['tm'] not in existing_times:
                    all_results.append(item)

            if len(items) < NUM_OF_ROWS:
                break
            page += 1

        print(f"[{date_obj}] {len(all_results)}ê±´ ìˆ˜ì§‘ ì„±ê³µ")
        return all_results

    def append_to_csv(new_data):
        """ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ CSVì— ì¶”ê°€"""
        if not new_data:
            return
        
        df = pd.DataFrame(new_data)
        
        # ê¸°ì¡´ íŒŒì¼ê³¼ ë™ì¼í•œ ì»¬ëŸ¼ ìˆœì„œë¡œ ì •ë ¬ (ìˆëŠ” ì»¬ëŸ¼ë§Œ)
        target_columns = ['tm', 'ta', 'rn', 'ws', 'wd', 'hm', 'pa', 'ps', 'td', 'pv']
        available_columns = [col for col in target_columns if col in df.columns]
        df = df[available_columns]
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë³€í™˜ (tm ì œì™¸)
        for col in available_columns:
            if col != 'tm':
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
        write_header = not os.path.exists(CSV_FILE)
        df.to_csv(CSV_FILE, mode='a', header=write_header, index=False)

    def get_last_collected_date():
        """ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì§‘ëœ ë‚ ì§œ í™•ì¸"""
        if not os.path.exists(CSV_FILE):
            return START_DATE
        try:
            df = pd.read_csv(CSV_FILE)
            last_time = pd.to_datetime(df['tm']).max()
            return last_time.date() + datetime.timedelta(days=1)
        except Exception as e:
            print(f"ë§ˆì§€ë§‰ ìˆ˜ì§‘ì¼ í™•ì¸ ì‹¤íŒ¨: {e}")
            return START_DATE

    # ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
    print("ì‹œê°„ë³„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    existing_times = load_existing_times()
    current_date = get_last_collected_date()
    
    print(f"í˜„ì¬ ìˆ˜ì§‘ëœ ë°ì´í„°: {len(existing_times):,}ê°œ ì‹œê°„ëŒ€")
    print(f"ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ: {current_date}")
    print(f"ìˆ˜ì§‘ ì¢…ë£Œ ë‚ ì§œ: {END_DATE}")

    if current_date > END_DATE:
        print("ì´ë¯¸ ìµœì‹  ë°ì´í„°ê¹Œì§€ ëª¨ë‘ ìˆ˜ì§‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        return

    collected_count = 0
    while current_date <= END_DATE:
        new_data = fetch_day_data(current_date, existing_times)
        if new_data:
            append_to_csv(new_data)
            collected_count += len(new_data)
            
            # ìƒˆë¡œ ìˆ˜ì§‘í•œ ë°ì´í„°ì˜ ì‹œê°„ì„ ê¸°ì¡´ setì— ì¶”ê°€
            for item in new_data:
                existing_times.add(item['tm'])
            
        current_date += datetime.timedelta(days=1)
        time.sleep(0.5)  # API ê³¼ë¶€í•˜ ë°©ì§€

    if collected_count == 0:
        print("ìƒˆë¡œìš´ ì‹œê°„ë³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤!")
    else:
        print(f"ì‹œê°„ë³„ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! (ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„°: {collected_count:,}ê±´)")


# 2. ì¼ë³„ ê´€ì¸¡ìë£Œ ìˆ˜ì§‘ (ASOS)
def get_daily_data():
    """ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘"""
    CSV_FILE = os.path.join(BASE_DIR, 'asos_seoul_daily.csv')
    API_URL = "http://apis.data.go.kr/1360000/AsosDalyInfoService/getWthrDataList"

    def load_existing_dates():
        """ê¸°ì¡´ì— ìˆ˜ì§‘ëœ ë‚ ì§œ ë°ì´í„° ë¡œë“œ"""
        if not os.path.exists(CSV_FILE):
            return set()
        try:
            df = pd.read_csv(CSV_FILE)
            return set(pd.to_datetime(df['tm']).dt.date.astype(str))
        except Exception as e:
            print(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return set()

    def fetch_day_data(date_obj):
        """íŠ¹ì • ë‚ ì§œì˜ ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘"""
        params = {
            'serviceKey': SERVICE_KEY,
            'numOfRows': NUM_OF_ROWS,
            'pageNo': 1,
            'dataCd': 'ASOS',
            'dateCd': 'DAY',
            'startDt': date_obj.strftime("%Y%m%d"),
            'endDt': date_obj.strftime("%Y%m%d"),
            'stnIds': STN_ID,
            'dataType': 'JSON'
        }

        try:
            response = requests.get(API_URL, params=params, timeout=60)
            response.raise_for_status()
            data = response.json()
            
            result_code = data['response']['header']['resultCode']
            if result_code != '00':
                print(f"[{date_obj}] API ì˜¤ë¥˜: {data['response']['header']['resultMsg']}")
                return []
                
            items = data['response']['body']['items'].get('item', [])
            return items
        except Exception as e:
            print(f"[{date_obj}] ì—ëŸ¬ ë°œìƒ: {e}")
            return []

    def append_to_csv(items):
        """ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ CSVì— ì¶”ê°€"""
        if not items:
            return

        df = pd.DataFrame(items)

        # ê¸°ì¡´ íŒŒì¼ê³¼ ë™ì¼í•œ ì»¬ëŸ¼ ìˆœì„œë¡œ ì •ë ¬ (ìˆëŠ” ì»¬ëŸ¼ë§Œ)
        target_columns = [
            'tm', 'avgTa', 'minTa', 'maxTa', 'sumRn', 'avgWs', 'avgRhm',
            'avgTs', 'ddMefs', 'sumGsr', 'maxInsWs', 'sumSmlEv', 'avgTd', 'avgPs'
        ]
        available_columns = [col for col in target_columns if col in df.columns]
        df = df[available_columns]

        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë³€í™˜ (tm ì œì™¸)
        for col in available_columns:
            if col != 'tm':
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # CSV ì €ì¥
        write_header = not os.path.exists(CSV_FILE)
        df.to_csv(CSV_FILE, mode='a', header=write_header, index=False)

    def get_last_collected_date():
        """ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì§‘ëœ ë‚ ì§œ í™•ì¸"""
        if not os.path.exists(CSV_FILE):
            return START_DATE
        try:
            df = pd.read_csv(CSV_FILE)
            last_date = pd.to_datetime(df['tm']).max().date()
            return last_date + datetime.timedelta(days=1)
        except Exception as e:
            print(f"ë§ˆì§€ë§‰ ìˆ˜ì§‘ì¼ í™•ì¸ ì‹¤íŒ¨: {e}")
            return START_DATE

    # ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
    print("ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    existing_dates = load_existing_dates()
    current_date = get_last_collected_date()
    
    print(f"í˜„ì¬ ìˆ˜ì§‘ëœ ë°ì´í„°: {len(existing_dates):,}ì¼")
    print(f"ìˆ˜ì§‘ ì‹œì‘ ë‚ ì§œ: {current_date}")
    print(f"ìˆ˜ì§‘ ì¢…ë£Œ ë‚ ì§œ: {END_DATE}")

    if current_date > END_DATE:
        print("ì´ë¯¸ ìµœì‹  ë°ì´í„°ê¹Œì§€ ëª¨ë‘ ìˆ˜ì§‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        return

    collected_count = 0
    while current_date <= END_DATE:
        if str(current_date) in existing_dates:
            current_date += datetime.timedelta(days=1)
            continue

        items = fetch_day_data(current_date)
        if items:
            append_to_csv(items)
            collected_count += len(items)
            print(f"[{current_date}] ì €ì¥ ì™„ë£Œ")
        else:
            print(f"[{current_date}] ë°ì´í„° ì—†ìŒ")

        current_date += datetime.timedelta(days=1)
        time.sleep(0.3)  # API ê³¼ë¶€í•˜ ë°©ì§€

    if collected_count == 0:
        print("ìƒˆë¡œìš´ ì¼ë³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤!")
    else:
        print(f"ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! (ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„°: {collected_count:,}ê±´)")


# 3. ë°ì´í„° ìƒíƒœ í™•ì¸ í•¨ìˆ˜
def check_data_status():
    """í˜„ì¬ ìˆ˜ì§‘ëœ ë°ì´í„° ìƒíƒœ í™•ì¸"""
    print("=== ë°ì´í„° ìƒíƒœ í™•ì¸ ===")
    
    # ì‹œê°„ë³„ ë°ì´í„° ìƒíƒœ
    hourly_file = os.path.join(BASE_DIR, 'asos_seoul_hourly.csv')
    if os.path.exists(hourly_file):
        df_hourly = pd.read_csv(hourly_file)
        hourly_dates = pd.to_datetime(df_hourly['tm'])
        print(f"ì‹œê°„ë³„ ë°ì´í„° (asos_seoul_hourly.csv):")
        print(f"   - ì´ {len(df_hourly):,}ê±´")
        print(f"   - ê¸°ê°„: {hourly_dates.min()} ~ {hourly_dates.max()}")
        print(f"   - ìµœê·¼ ë°ì´í„°: {hourly_dates.max()}")
        print(f"   - ì»¬ëŸ¼: {list(df_hourly.columns)}")
    else:
        print("ì‹œê°„ë³„ ë°ì´í„°: íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì¼ë³„ ë°ì´í„° ìƒíƒœ  
    daily_file = os.path.join(BASE_DIR, 'asos_seoul_daily.csv')
    if os.path.exists(daily_file):
        df_daily = pd.read_csv(daily_file)
        daily_dates = pd.to_datetime(df_daily['tm'])
        print(f"ì¼ë³„ ë°ì´í„° (asos_seoul_daily.csv):")
        print(f"   - ì´ {len(df_daily):,}ê±´")
        print(f"   - ê¸°ê°„: {daily_dates.min().date()} ~ {daily_dates.max().date()}")
        print(f"   - ìµœê·¼ ë°ì´í„°: {daily_dates.max().date()}")
        print(f"   - ì»¬ëŸ¼: {list(df_daily.columns)}")
    else:
        print("ì¼ë³„ ë°ì´í„°: íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    print()


# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def preprocess_data():
    """ì „ì²´ ë‚ ì”¨ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)"""
    print("=== ì„œìš¸ ë‚ ì”¨ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
    
    # 0. í˜„ì¬ ë°ì´í„° ìƒíƒœ í™•ì¸
    print("\n[0ë‹¨ê³„] í˜„ì¬ ë°ì´í„° ìƒíƒœ í™•ì¸...")
    check_data_status()
    
    # 1. ë°ì´í„° ìˆ˜ì§‘ (ì¦ë¶„ ì—…ë°ì´íŠ¸)
    print("\n[1ë‹¨ê³„] ì‹œê°„ë³„/ì¼ë³„ ë‚ ì”¨ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    get_hourly_data()
    print()
    get_daily_data()
    
    # 2. ì—…ë°ì´íŠ¸ í›„ ë°ì´í„° ìƒíƒœ ì¬í™•ì¸
    print("\n[2ë‹¨ê³„] ì—…ë°ì´íŠ¸ í›„ ë°ì´í„° ìƒíƒœ í™•ì¸...")
    check_data_status()
    
    print("=== ì „ì²´ ì‘ì—… ì™„ë£Œ! ===")
    print(f"ğŸ“ ë°ì´í„° ì €ì¥ ìœ„ì¹˜: {os.path.abspath(BASE_DIR)}/")


def collect_weather_data():
    """ì „ì²´ ë‚ ì”¨ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ (ë³„ì¹­)"""
    preprocess_data()


def update_data_only():
    """ë°ì´í„° ìˆ˜ì§‘ë§Œ ì‹¤í–‰"""
    print("=== ë°ì´í„° ì—…ë°ì´íŠ¸ ëª¨ë“œ ===")
    check_data_status()
    print("ìƒˆë¡œìš´ ë°ì´í„° í™•ì¸ ì¤‘...")
    get_hourly_data()
    print()
    get_daily_data()
    print("\nì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    check_data_status()


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "check":
            check_data_status()
        elif sys.argv[1] == "update":
            update_data_only()
        elif sys.argv[1] == "collect":
            preprocess_data()
        else:
            print("ì‚¬ìš©ë²•:")
            print("  python preprocessor.py check   - ë°ì´í„° ìƒíƒœë§Œ í™•ì¸")
            print("  python preprocessor.py update  - ìƒˆë¡œìš´ ë°ì´í„°ë§Œ ìˆ˜ì§‘")
            print("  python preprocessor.py collect - ì „ì²´ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰")
    else:
        # ê¸°ë³¸ ì‹¤í–‰: ì „ì²´ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤
        preprocess_data()